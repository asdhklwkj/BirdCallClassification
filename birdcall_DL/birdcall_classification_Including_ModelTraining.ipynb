{"cells":[{"cell_type":"markdown","source":["## Deep Learning-Based Birdcall Classification"],"metadata":{"id":"1NPcCrNgpxl8"},"id":"1NPcCrNgpxl8"},{"cell_type":"code","source":["import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import os\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set the random number generator seed for all modules.\"\"\"\n","    random.seed(seed_value)  # Built-in Python random module\n","    np.random.seed(seed_value)  # NumPy\n","    torch.manual_seed(seed_value)  # PyTorch function for CPUs\n","    torch.cuda.manual_seed(seed_value)  # PyTorch function for GPUs\n","    torch.cuda.manual_seed_all(seed_value)  # PyTorch function for multi-GPUs\n","    torch.backends.cudnn.deterministic = True  # Makes cudnn algorithm deterministic\n","    torch.backends.cudnn.benchmark = False  # Disables the cudnn benchmarking\n","    os.environ['PYTHONHASHSEED'] = str(seed_value)\n","\n","set_seed(42)  # Call with the desired seed value\n"],"metadata":{"id":"YFw49s6Z8T0g"},"id":"YFw49s6Z8T0g","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install python_speech_features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvqLPqJ435kp","outputId":"febd15b8-a535-4d50-9194-b85b3abf6ef6","executionInfo":{"status":"ok","timestamp":1713548797054,"user_tz":-60,"elapsed":19999,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"id":"lvqLPqJ435kp","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python_speech_features\n","  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: python_speech_features\n","  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5870 sha256=f888c04f58b35491cb68d20f6db00e7dca2ca0dc4c6fb93f7d039ffaa54a5f7f\n","  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n","Successfully built python_speech_features\n","Installing collected packages: python_speech_features\n","Successfully installed python_speech_features-0.6\n"]}]},{"cell_type":"code","execution_count":null,"id":"c52d1633","metadata":{"id":"c52d1633"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.io.wavfile import read\n","from IPython.lib.display import Audio\n","from scipy.fftpack import fft, ifft\n","from scipy.io import loadmat\n","import scipy.signal as sgnl\n","import scipy.io.wavfile as wav\n","import sys\n","import wave\n","import operator\n","import scipy\n","from python_speech_features import mfcc"]},{"cell_type":"markdown","source":["## Model Training\n","\n","\n","---\n","Dataset Preparation: Construct a dataset that includes results from MFCC (Mel Frequency Cepstral Coefficients) and Fourier Transforms.\n","\n","Model Definition: Define models such as LSTM (Long Short-Term Memory) or CNN (Convolutional Neural Network).\n","\n","Model Training: Use the dataset to train the model.\n","\n","Inference and Evaluation: Perform inference using the trained model and evaluate its performance."],"metadata":{"id":"44Oz4Bsun8nh"},"id":"44Oz4Bsun8nh"},{"cell_type":"markdown","source":["Library Installation and Import"],"metadata":{"id":"f09VXbvNoUU4"},"id":"f09VXbvNoUU4"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#Setting the Current Directory\n","%cd /content/drive/My Drive/birdcall"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zyBQYSE2bQU","outputId":"5fec325a-be77-45e9-ab81-71c5901dc0dd","executionInfo":{"status":"ok","timestamp":1713548889186,"user_tz":-60,"elapsed":65619,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"id":"_zyBQYSE2bQU","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/birdcall\n"]}]},{"cell_type":"markdown","source":["데이터 설정 및 전처리 mfcc 활용"],"metadata":{"id":"UVzZwEtB3czo"},"id":"UVzZwEtB3czo"},{"cell_type":"markdown","source":["Model Definition and Training\n","\n","---\n","LSTM Classifier"],"metadata":{"id":"A_IGU1na3fLh"},"id":"A_IGU1na3fLh"},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","# Check if CUDA is available and set the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFJDaNDY8LJT","outputId":"165fa581-52a4-4e02-a26d-4d5e856d333f","executionInfo":{"status":"ok","timestamp":1713548893250,"user_tz":-60,"elapsed":302,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"id":"QFJDaNDY8LJT","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"markdown","source":["## Audio Data Preprocessing Steps\n","1. Convert MP3 data to WAV format.\n","2. Perform Fourier Transform and high-pass filtering.\n","3. Extract MFCC (Mel Frequency Cepstral Coefficients)\n","4. Transform the data  into a format suitable for input into the model."],"metadata":{"id":"ZuBqxsDgDd7a"},"id":"ZuBqxsDgDd7a"},{"cell_type":"markdown","source":["# Setting Up Training Data"],"metadata":{"id":"6XKU7rx4D7XJ"},"id":"6XKU7rx4D7XJ"},{"cell_type":"code","source":["import os\n","import shutil\n","import glob\n","import random\n","\n","# Retrieve the list of .wav files from the current directory.\n","source_dir = './wav'  # Change this to the directory path where your .wav files are located.\n","train_dir = './train'\n","val_dir = './val'\n","\n","# Create train and val directories if they do not exist.\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","\n","# Load the list of .wav files.\n","wav_files = glob.glob(os.path.join(source_dir, '*.wav'))\n","\n","# Randomly shuffle the list of files.\n","random.shuffle(wav_files)\n","\n","# Split the files into an 80:20 ratio.\n","split_index = int(len(wav_files) * 0.8)\n","train_files = wav_files[:split_index]\n","val_files = wav_files[split_index:]\n","\n","# Move the files to their respective directories.\n","for f in train_files:\n","    shutil.move(f, os.path.join(train_dir, os.path.basename(f)))\n","\n","for f in val_files:\n","    shutil.move(f, os.path.join(val_dir, os.path.basename(f)))\n","\n","print(f'Moved {len(train_files)} files to {train_dir}')\n","print(f'Moved {len(val_files)} files to {val_dir}')\n"],"metadata":{"id":"e-efNqrz9_O0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f36cf48-4612-44a4-e037-e7badaee906e","executionInfo":{"status":"ok","timestamp":1713548908130,"user_tz":-60,"elapsed":10473,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"id":"e-efNqrz9_O0","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Moved 619 files to ./train\n","Moved 155 files to ./val\n"]}]},{"cell_type":"code","source":["!pip install pydub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoMv4GQ7Vz2C","outputId":"dc23d5e6-386c-44a6-961f-8430359a7e4d","executionInfo":{"status":"ok","timestamp":1713549016106,"user_tz":-60,"elapsed":7505,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"id":"KoMv4GQ7Vz2C","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n"]}]},{"cell_type":"markdown","source":["Convert MP3 files to WAV files and save them by dividing them into train and val files with a 9:1 ratio"],"metadata":{"id":"uSK5qCrxtv0_"},"id":"uSK5qCrxtv0_"},{"cell_type":"markdown","source":["#Preprocessing\n","---\n","\n","\n","1. Load WAV files and generate labels through file names.\n","2. Perform Fourier Transform.\n","3. Load and apply a low-pass filter.\n","4. Extract MFCC (Mel Frequency Cepstral Coefficients) features.\n","5. Transform the data into a format suitable for model training."],"metadata":{"id":"YJbDZn_AOBKs"},"id":"YJbDZn_AOBKs"},{"cell_type":"code","source":["import os\n","import numpy as np\n","from scipy.io import wavfile, loadmat\n","from scipy.fft import fft, fftfreq, ifftshift\n","from scipy.fftpack import fftshift\n","from python_speech_features import mfcc\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","def apply_fourier_transform(input_audio, Fs):\n","    N = len(input_audio)\n","    f_transform = fftshift(fft(input_audio, N))\n","    frequencies = np.linspace(-Fs/2, Fs/2, N)\n","    return f_transform, frequencies\n","\n","import scipy.signal as signal\n","\n","def apply_lowpass_filter(input_audio, b, a):\n","    filtered_audio = signal.lfilter(b, a, input_audio)\n","    return filtered_audio\n","\n","def extract_mfcc_features(filtered_audio, Fs):\n","    mfcc_features = mfcc(filtered_audio, Fs)\n","    return mfcc_features\n","\n","# Load low-pass filter\n","filter_data = loadmat('./low_filter/highpass(500).mat') # load the filter coefficients\n","Coeffs = filter_data['ba'].astype(np.float64) # obtaining filter coefficients\n","b = Coeffs[0,:] # first column is b\n","a = 1\n","\n","# Dictionary to map labels to their indices\n","labels_index = {}\n","\n","wav_files_dir = './train'  # Directory where WAV files are stored\n","wav_files = [f for f in os.listdir(wav_files_dir) if f.endswith('.wav')]\n","\n","features_list = []  # List to store features\n","labels_list = []  # List to store labels\n","\n","for wav_file in wav_files:\n","    # Extract label (remove .wav and last '_number')\n","    label = '_'.join(wav_file.split('_')[:-1])\n","    if label not in labels_index:\n","        labels_index[label] = len(labels_index)\n","    label_index = labels_index[label]\n","\n","    file_path = os.path.join(wav_files_dir, wav_file)\n","\n","    # Load WAV file\n","    Fs, input_audio = wavfile.read(file_path)\n","\n","    # Apply low-pass filtering\n","    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n","\n","    # Extract MFCC\n","    mfcc_features = extract_mfcc_features(filtered_audio, Fs)\n","\n","    # Add to list\n","    features_list.append(mfcc_features)\n","    labels_list.append(label_index)\n","\n","# Calculate the shortest length\n","min_length = min([len(feat) for feat in features_list])\n","\n","# Trim all features to the shortest length\n","trimmed_features_list = [feat[:min_length] for feat in features_list]\n","\n","# Convert features and labels to NumPy arrays\n","features_array = np.array(trimmed_features_list)\n","labels_array = np.array(labels_list)\n","\n","# Convert to PyTorch tensors\n","features_tensor = torch.tensor(features_array, dtype=torch.float)\n","labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n","\n","# Create TensorDataset\n","dataset = TensorDataset(features_tensor, labels_tensor)\n","\n","# Create DataLoader\n","dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"],"metadata":{"id":"5d-y_2kKOJE1"},"id":"5d-y_2kKOJE1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 매핑 확인\n","for label, index in labels_index.items():\n","    print(f\"Label '{label}' is mapped to index {index}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4kh0cbGDOj68","outputId":"fb1c788b-be99-4d96-a9be-472ca50dd6ef","executionInfo":{"status":"ok","timestamp":1713549079501,"user_tz":-60,"elapsed":4,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"id":"4kh0cbGDOj68","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Label 'Velvet_Scoter_Melanitta_fusca' is mapped to index 0\n","Label 'Long_tailed_Duck_Clangula_hyemalis' is mapped to index 1\n","Label 'Leach's_Storm_Petrel_Hydrobates_leucorhous' is mapped to index 2\n","Label 'European_Turtle_Dove_Streptopelia_turtur' is mapped to index 3\n","Label 'Black_legged_Kittiwake_Rissa_tridactyla' is mapped to index 4\n","Label 'Balearic_Shearwater_Puffinus_mauretanicus' is mapped to index 5\n","Label 'Atlantic_Puffin_Fratercula_arctica' is mapped to index 6\n","Label 'Aquatic_Warbler_Acrocephalus_paludicola' is mapped to index 7\n","Label 'Great_Bustard_Otis_tarda' is mapped to index 8\n","Label 'Horned_Grebe_Podiceps_auritus' is mapped to index 9\n"]}]},{"cell_type":"code","source":["# Load Validation Data\n","val_dir = './val'  # Directory where validation data is stored\n","val_wav_files = [f for f in os.listdir(val_dir) if f.endswith('.wav')]\n","\n","val_features_list = []  # List to store features of validation data\n","val_labels_list = []  # List to store labels of validation data\n","\n","for wav_file in val_wav_files:\n","    label = '_'.join(wav_file.split('_')[:-1])  # Extract label from file name\n","    if label not in labels_index:  # Skip if the label is not in the training data label index\n","        continue  # Move to the next file\n","\n","    file_path = os.path.join(val_dir, wav_file)\n","    # Load WAV file\n","    Fs, input_audio = wavfile.read(file_path)\n","\n","    # Apply low-pass filtering\n","    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n","\n","    # Extract MFCC\n","    mfcc_features = extract_mfcc_features(filtered_audio, Fs)\n","\n","    label_index = labels_index[label]  # Convert label name to label index\n","    val_features_list.append(mfcc_features)  # Add extracted features to the list\n","    val_labels_list.append(label_index)  # Add label index to the list\n","\n","# Calculate the shortest length\n","min_length = min([len(feat) for feat in val_features_list])\n","\n","# Trim all features to the shortest length\n","trimmed_val_features_list = [feat[:min_length] for feat in val_features_list]\n","\n","# Convert features and labels to NumPy arrays\n","val_features_array = np.array(trimmed_val_features_list)\n","val_labels_array = np.array(val_labels_list)\n","\n","# Convert to PyTorch tensors\n","val_features_tensor = torch.tensor(val_features_array, dtype=torch.float)\n","val_labels_tensor = torch.tensor(val_labels_array, dtype=torch.long)\n","\n","val_dataset = TensorDataset(val_features_tensor, val_labels_tensor)\n","val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True)"],"metadata":{"id":"ilmbGqSt53D9"},"id":"ilmbGqSt53D9","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the Model"],"metadata":{"id":"FnE1V-MSUblb"},"id":"FnE1V-MSUblb"},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class LSTMSoundClassifier(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes, num_layers=3):  # Default value for num_layers set to 3\n","        super(LSTMSoundClassifier, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # Define LSTM layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","\n","        # Define additional fully connected layers\n","        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)  # Fully connected layer that doubles the hidden size\n","        self.fc2 = nn.Linear(hidden_size * 2, num_classes)  # Layer for final output\n","\n","        # Add dropout (common usage)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        # Initial hidden and cell states\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","\n","        # Pass through LSTM layers\n","        out, _ = self.lstm(x, (h0, c0))\n","\n","        # Use dropout\n","        out = self.dropout(out[:, -1, :])  # applying dropout to the output of the last LSTM layer\n","\n","        # Pass through fully connected layers\n","        out = F.relu(self.fc1(out))\n","        out = self.fc2(out)\n","\n","        return out"],"metadata":{"id":"_RG7av2hqfYb"},"id":"_RG7av2hqfYb","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Training"],"metadata":{"id":"F2bK-4N_UnD4"},"id":"F2bK-4N_UnD4"},{"cell_type":"code","source":["import torch.optim as optim\n","import torch.nn.functional as F\n","import copy\n","\n","# Set up the model, loss function, and optimizer\n","hidden_size = 512  # Adjust hidden_size to a larger value to deepen the model\n","model = LSTMSoundClassifier(input_size=features_tensor.size(2), hidden_size=hidden_size, num_classes=len(labels_index)).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(model.parameters(), lr=0.0005)\n","\n","num_epochs = 50\n","\n","best_model_wts = copy.deepcopy(model.state_dict())\n","best_acc = 0.0\n","patience = 5  # Number of epochs to allow without performance improvement\n","patience_counter = 0  # Counter for epochs without performance improvement\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","\n","    for features, labels in dataloader:\n","        features, labels = features.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(features)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        _, predictions = torch.max(outputs, 1)\n","        train_loss += loss.item()\n","        train_correct += (predictions == labels).sum().item()\n","        train_total += labels.size(0)\n","\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        for features, labels in val_dataloader:\n","            features, labels = features.to(device), labels.to(device)\n","            outputs = model(features)\n","            loss = criterion(outputs, labels)\n","\n","            _, predictions = torch.max(outputs, 1)\n","            val_loss += loss.item()\n","            val_correct += (predictions == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    epoch_acc = val_correct / val_total\n","\n","    # Save the best model\n","    if epoch_acc > best_acc:\n","        best_acc = epoch_acc\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","        patience_counter = 0  # Reset counter on performance improvement\n","    else:\n","        patience_counter += 1  # Increase counter if no performance improvement\n","\n","    print(f'Epoch {epoch+1}: Train Loss: {train_loss / len(dataloader):.4f}, Train Accuracy: {train_correct / train_total:.4f}, '\n","          f'Val Loss: {val_loss / len(val_dataloader):.4f}, Val Accuracy: {val_correct / val_total:.4f}')\n","\n","    # Check for early stopping condition\n","    if patience_counter == patience:\n","        print(\"Early stopping\")\n","        break\n","\n","# Restore the best model (code to save the best performing model)\n","model.load_state_dict(best_model_wts)\n","torch.save(model.state_dict(), '4.19.pth') # model name is adjustable."],"metadata":{"id":"3FcS1Fb62EZx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bb9c7f5a-3e67-41d5-9911-36e86360594d","executionInfo":{"status":"ok","timestamp":1713549144455,"user_tz":-60,"elapsed":29501,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"id":"3FcS1Fb62EZx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss: 1.6926, Train Accuracy: 0.4443, Val Loss: 1.7684, Val Accuracy: 0.5665\n","Epoch 2: Train Loss: 1.2363, Train Accuracy: 0.5987, Val Loss: 1.3113, Val Accuracy: 0.6897\n","Epoch 3: Train Loss: 1.0303, Train Accuracy: 0.6792, Val Loss: 1.3685, Val Accuracy: 0.7192\n","Epoch 4: Train Loss: 0.9204, Train Accuracy: 0.7034, Val Loss: 1.7142, Val Accuracy: 0.6749\n","Epoch 5: Train Loss: 0.8579, Train Accuracy: 0.7195, Val Loss: 1.5537, Val Accuracy: 0.7635\n","Epoch 6: Train Loss: 0.7886, Train Accuracy: 0.7503, Val Loss: 1.3924, Val Accuracy: 0.7635\n","Epoch 7: Train Loss: 0.7113, Train Accuracy: 0.7584, Val Loss: 1.7369, Val Accuracy: 0.7833\n","Epoch 8: Train Loss: 0.6507, Train Accuracy: 0.7987, Val Loss: 1.6134, Val Accuracy: 0.7537\n","Epoch 9: Train Loss: 0.5489, Train Accuracy: 0.8134, Val Loss: 1.6567, Val Accuracy: 0.7833\n","Epoch 10: Train Loss: 0.5304, Train Accuracy: 0.8295, Val Loss: 1.8698, Val Accuracy: 0.7783\n","Epoch 11: Train Loss: 0.4593, Train Accuracy: 0.8470, Val Loss: 1.9129, Val Accuracy: 0.7833\n","Epoch 12: Train Loss: 0.3562, Train Accuracy: 0.8859, Val Loss: 2.1292, Val Accuracy: 0.7833\n","Early stopping\n"]}]},{"cell_type":"markdown","source":["## Algorithm to print new name and its probability with a test sample"],"metadata":{"id":"An0GmhiG2c7v"},"id":"An0GmhiG2c7v"},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# Load test data sample\n","# Define functions for low-pass filtering and extracting MFCC\n","def preprocess_audio(file_path, b, a):\n","    # Load WAV file\n","    Fs, input_audio = wavfile.read(file_path)\n","\n","    # Apply low-pass filtering\n","    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n","\n","    # Extract MFCC\n","    mfcc_features = mfcc(filtered_audio, samplerate=Fs, numcep=13)\n","\n","    return mfcc_features\n","\n","# Test sample file path\n","test_file_path = './test/Black_Legged_Kittiwake_2_TEST.wav'\n","\n","# Apply preprocessing to the test sample\n","test_feature = preprocess_audio(test_file_path, b, a)\n","\n","# Convert to PyTorch tensor\n","test_feature_tensor = torch.tensor([test_feature], dtype=torch.float)#.to(device)\n","\n","# Define model\n","model = LSTMSoundClassifier(input_size=test_feature_tensor.size(2), hidden_size=512, num_classes=len(labels_index))#.to(device)\n","model.load_state_dict(torch.load('./3.23.pth'))\n","######model.load_state_dict(torch.load('./best_model_1.pth'))\n","model.eval()  # Set to inference mode\n","threshold = 0.5  # Set the threshold for probability\n","\n","# Perform inference on the test sample\n","with torch.no_grad():\n","    output = model(test_feature_tensor)  # test_feature_tensor is the preprocessed test data\n","    probabilities = F.softmax(output, dim=1)  # Convert model output to probabilities using softmax\n","    max_probs, predicted_indices = torch.max(probabilities, dim=1)  # Extract the maximum probability and corresponding index for each sample\n","\n","    # Handle the first sample\n","    predicted_prob = max_probs.item()  # Maximum probability of the first sample\n","    predicted_index = predicted_indices.item()  # Predicted class index of the first sample\n","\n","    # Treat predictions with probability lower than the threshold as 'non-match'\n","    if predicted_prob < threshold:\n","        print(\"Non-match\")\n","    else:\n","        # Retrieve the name of the predicted class\n","        predicted_label = [label for label, index in labels_index.items() if index == predicted_index][0]\n","        print(f\"Predicted label: {predicted_label}, with probability: {predicted_prob:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CXkQZt9S2gQ6","outputId":"7e0da366-7884-4a38-95a6-8c68337ed323"},"id":"CXkQZt9S2gQ6","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted label: Black_legged_Kittiwake_Rissa_tridactyla, with probability: 0.8980\n"]}]},{"cell_type":"code","source":["print(predicted_prob)"],"metadata":{"id":"xXDomZC27SNJ"},"id":"xXDomZC27SNJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","# Load test data sample\n","# Define functions for low-pass filtering and extracting MFCC\n","def preprocess_audio(file_path, b, a):\n","    # Load WAV file\n","    Fs, input_audio = wavfile.read(file_path)\n","\n","    # Apply low-pass filtering\n","    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n","\n","    # Extract MFCC\n","    mfcc_features = mfcc(filtered_audio, samplerate=Fs, numcep=13)\n","\n","    return mfcc_features\n","\n","# Path of the test sample file\n","test_file_path = './test/Long_tailed_duck_2_TEST.wav'\n","\n","# Apply preprocessing to the test sample\n","test_feature = preprocess_audio(test_file_path, b, a)\n","\n","# Convert to PyTorch tensor\n","test_feature_tensor = torch.tensor([test_feature], dtype=torch.float).to(device)\n","\n","# Define the model\n","model = LSTMSoundClassifier(input_size=test_feature_tensor.size(2), hidden_size=256, num_classes=len(labels_index)).to(device)\n","model.load_state_dict(torch.load('./best_model_1.pth'))\n","model.eval()  # Set to inference mode\n","threshold = 0.6  # Set the threshold for probability\n","\n","# Perform inference on the test sample\n","with torch.no_grad():\n","    output = model(test_feature_tensor)  # test_feature_tensor is the preprocessed test data\n","    probabilities = F.softmax(output, dim=1)  # Convert the model output to probabilities using the softmax function\n","    max_probs, predicted_indices = torch.max(probabilities, dim=1)  # Extract the maximum probability and corresponding index for each sample\n","\n","    # Handle the first sample\n","    predicted_prob = max_probs.item()  # Maximum probability of the first sample\n","    predicted_index = predicted_indices.item()  # Predicted class index of the first sample\n","\n","    # Treat predictions with a probability lower than the threshold as 'non-match'\n","    if predicted_prob < threshold:\n","        print(\"Non-match\")\n","    else:\n","        # Retrieve the name of the predicted class\n","        predicted_label = [label for label, index in labels_index.items() if index == predicted_index][0]\n","        print(f\"Predicted label: {predicted_label}, with probability: {predicted_prob:.4f}\")"],"metadata":{"id":"1BskWCocBUKy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f98b7b58-d844-47a5-af47-8255ee06a728"},"id":"1BskWCocBUKy","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted label: Black_legged_Kittiwake_Rissa_tridactyla, with probability: 0.9997\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"FKbTqlmnc88V"},"id":"FKbTqlmnc88V","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}