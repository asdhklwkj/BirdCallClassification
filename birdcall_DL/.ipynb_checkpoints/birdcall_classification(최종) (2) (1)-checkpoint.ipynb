{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1NPcCrNgpxl8",
   "metadata": {
    "id": "1NPcCrNgpxl8"
   },
   "source": [
    "## 딥러닝을 이용한 birdcall classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YFw49s6Z8T0g",
   "metadata": {
    "id": "YFw49s6Z8T0g"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"모든 모듈의 난수 생성기 시드를 고정합니다.\"\"\"\n",
    "    random.seed(seed_value)  # 파이썬 내장 random 모듈\n",
    "    np.random.seed(seed_value)  # NumPy\n",
    "    torch.manual_seed(seed_value)  # CPU를 위한 PyTorch 함수\n",
    "    torch.cuda.manual_seed(seed_value)  # GPU를 위한 PyTorch 함수\n",
    "    torch.cuda.manual_seed_all(seed_value)  # 멀티 GPU를 위한 PyTorch 함수\n",
    "    torch.backends.cudnn.deterministic = True  # cudnn 알고리즘의 동작을 결정적으로 만듦\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_seed(42)  # 원하는 시드 값으로 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lvqLPqJ435kp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12917,
     "status": "ok",
     "timestamp": 1709974777278,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "lvqLPqJ435kp",
    "outputId": "5475ad3a-ee81-4560-f7ec-a579c5e9d32d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python_speech_features\n",
      "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: python_speech_features\n",
      "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5869 sha256=ad049d8711a4bb8b31889fdbf85ad14f50024b796617738a762d24beeba4be12\n",
      "  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n",
      "Successfully built python_speech_features\n",
      "Installing collected packages: python_speech_features\n",
      "Successfully installed python_speech_features-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d1633",
   "metadata": {
    "id": "c52d1633"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read\n",
    "from IPython.lib.display import Audio\n",
    "from scipy.fftpack import fft, ifft\n",
    "from scipy.io import loadmat\n",
    "import scipy.signal as sgnl\n",
    "import scipy.io.wavfile as wav\n",
    "import sys\n",
    "import wave\n",
    "import operator\n",
    "import scipy\n",
    "from python_speech_features import mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44Oz4Bsun8nh",
   "metadata": {
    "id": "44Oz4Bsun8nh"
   },
   "source": [
    "## 모델 학습\n",
    "\n",
    "\n",
    "---\n",
    "데이터셋 준비: MFCC 및 푸리에 변환 결과를 포함하는 데이터셋을 구성합니다.\n",
    "\n",
    "모델 정의: LSTM이나 CNN 모델을 정의합니다.\n",
    "\n",
    "모델 학습: 데이터셋을 사용하여 모델을 학습합니다.\n",
    "\n",
    "추론 및 평가: 학습된 모델을 사용하여 추론을 수행하고 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09VXbvNoUU4",
   "metadata": {
    "id": "f09VXbvNoUU4"
   },
   "source": [
    "라이브러리 설치 및 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_zyBQYSE2bQU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19924,
     "status": "ok",
     "timestamp": 1709974806919,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "_zyBQYSE2bQU",
    "outputId": "a673c7a6-7250-4604-971e-d4fb8925d25c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/birdcall\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# 현재 디렉토리 설정\n",
    "%cd /content/drive/My Drive/birdcall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UVzZwEtB3czo",
   "metadata": {
    "id": "UVzZwEtB3czo"
   },
   "source": [
    "데이터 설정 및 전처리 mfcc 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A_IGU1na3fLh",
   "metadata": {
    "id": "A_IGU1na3fLh"
   },
   "source": [
    "모델 정의 및 학습\n",
    "\n",
    "---\n",
    "lstm 분류 (직접 코드 작성한 것) 따로 참고한 것 x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QFJDaNDY8LJT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1709974812635,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "QFJDaNDY8LJT",
    "outputId": "d2cc4ed9-820c-45b6-df6b-84cad314874c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# CUDA 사용 가능 여부 확인 및 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZuBqxsDgDd7a",
   "metadata": {
    "id": "ZuBqxsDgDd7a"
   },
   "source": [
    "## 음성 데이터 전처리 과정\n",
    "1. mp3 데이터 wav로 변환\n",
    "2. 푸리에 변환 및 저주파 필터링\n",
    "3. MFCC 추출\n",
    "4. 모델에 input하기 적합한 형태로 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6XKU7rx4D7XJ",
   "metadata": {
    "id": "6XKU7rx4D7XJ"
   },
   "source": [
    "# 학습 데이터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e-efNqrz9_O0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6353,
     "status": "ok",
     "timestamp": 1709975081453,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "e-efNqrz9_O0",
    "outputId": "d60a4cdb-755a-4e3e-8230-4c9477fe2267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 697 files to ./train\n",
      "Moved 78 files to ./val\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# 현재 디렉토리의 .wav 파일 목록을 가져옵니다.\n",
    "source_dir = './wav'  # 이곳을 .wav 파일들이 있는 디렉토리 경로로 변경하세요.\n",
    "train_dir = './train'\n",
    "val_dir = './val'\n",
    "\n",
    "# train과 val 디렉토리가 없으면 생성합니다.\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# .wav 파일 목록을 불러옵니다.\n",
    "wav_files = glob.glob(os.path.join(source_dir, '*.wav'))\n",
    "\n",
    "# 파일 목록을 무작위로 섞습니다.\n",
    "random.shuffle(wav_files)\n",
    "\n",
    "# 9:1 비율로 분할합니다.\n",
    "split_index = int(len(wav_files) * 0.9)\n",
    "train_files = wav_files[:split_index]\n",
    "val_files = wav_files[split_index:]\n",
    "\n",
    "# 파일을 각각의 디렉토리로 이동시킵니다.\n",
    "for f in train_files:\n",
    "    shutil.move(f, os.path.join(train_dir, os.path.basename(f)))\n",
    "\n",
    "for f in val_files:\n",
    "    shutil.move(f, os.path.join(val_dir, os.path.basename(f)))\n",
    "\n",
    "print(f'Moved {len(train_files)} files to {train_dir}')\n",
    "print(f'Moved {len(val_files)} files to {val_dir}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KoMv4GQ7Vz2C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6869,
     "status": "ok",
     "timestamp": 1709816465451,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "KoMv4GQ7Vz2C",
    "outputId": "7568a16b-b7ff-45f8-b16a-23c23efb1df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uSK5qCrxtv0_",
   "metadata": {
    "id": "uSK5qCrxtv0_"
   },
   "source": [
    "mp3 파일을 wav파일로 바꿔서 9:1 비율로 train 파일과 val 파일로 나눠서 저장!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YJbDZn_AOBKs",
   "metadata": {
    "id": "YJbDZn_AOBKs"
   },
   "source": [
    "#전처리\n",
    "---\n",
    "\n",
    "\n",
    "1. wav 파일 불러와서 파일이름을 통해서 라벨도 같이 생성\n",
    "2. 푸리에 변환\n",
    "3. 저주파 필터 로드해서 변환\n",
    "4. mfcc 특징 추출\n",
    "5. 모델 학습을 위한 데이터 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d-y_2kKOJE1",
   "metadata": {
    "id": "5d-y_2kKOJE1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile, loadmat\n",
    "from scipy.fft import fft, fftfreq, ifftshift\n",
    "from scipy.fftpack import fftshift\n",
    "from python_speech_features import mfcc\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def apply_fourier_transform(input_audio, Fs):\n",
    "    N = len(input_audio)\n",
    "    f_transform = fftshift(fft(input_audio, N))\n",
    "    frequencies = np.linspace(-Fs/2, Fs/2, N)\n",
    "    return f_transform, frequencies\n",
    "\n",
    "import scipy.signal as signal\n",
    "\n",
    "\n",
    "def apply_lowpass_filter(input_audio, b, a):\n",
    "    filtered_audio = signal.lfilter(b, a, input_audio)\n",
    "    return filtered_audio\n",
    "\n",
    "def extract_mfcc_features(filtered_audio, Fs):\n",
    "    mfcc_features = mfcc(filtered_audio, Fs)\n",
    "    return mfcc_features\n",
    "\n",
    "# 저주파 필터 로드\n",
    "filter_data = loadmat('./low_filter/NoiseFiltering_LowPass.mat') # load the filter obtained from␣\n",
    "Coeffs = filter_data['ba'].astype(np.float64) # getting filter coefficients\n",
    "b = Coeffs[0,:] # first column is b\n",
    "a = 1\n",
    "\n",
    "# 라벨과 해당 인덱스를 매핑하는 사전 정의\n",
    "labels_index = {}\n",
    "\n",
    "wav_files_dir = './train'  # WAV 파일이 저장된 디렉토리\n",
    "wav_files = [f for f in os.listdir(wav_files_dir) if f.endswith('.wav')]\n",
    "\n",
    "features_list = []  # 특성을 저장할 리스트\n",
    "labels_list = []  # 라벨을 저장할 리스트\n",
    "\n",
    "for wav_file in wav_files:\n",
    "    # 라벨 추출 (파일 이름에서 .wav 제거 및 마지막 '_번호' 제거)\n",
    "    label = '_'.join(wav_file.split('_')[:-1])\n",
    "    if label not in labels_index:\n",
    "        labels_index[label] = len(labels_index)\n",
    "    label_index = labels_index[label]\n",
    "\n",
    "    file_path = os.path.join(wav_files_dir, wav_file)\n",
    "\n",
    "    # WAV 파일 로드\n",
    "    Fs, input_audio = wavfile.read(file_path)\n",
    "\n",
    "    # 저주파 필터링 적용\n",
    "    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n",
    "\n",
    "    # MFCC 추출\n",
    "    mfcc_features = extract_mfcc_features(filtered_audio, Fs)\n",
    "\n",
    "    # 리스트에 추가\n",
    "    features_list.append(mfcc_features)\n",
    "    labels_list.append(label_index)\n",
    "\n",
    "# 가장 짧은 길이 계산\n",
    "min_length = min([len(feat) for feat in features_list])\n",
    "\n",
    "# 가장 짧은 길이로 모든 특성을 자르기\n",
    "trimmed_features_list = [feat[:min_length] for feat in features_list]\n",
    "\n",
    "# 특성과 라벨을 NumPy 배열로 변환\n",
    "features_array = np.array(trimmed_features_list)\n",
    "labels_array = np.array(labels_list)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "features_tensor = torch.tensor(features_array, dtype=torch.float)\n",
    "labels_tensor = torch.tensor(labels_array, dtype=torch.long)\n",
    "\n",
    "\n",
    "# TensorDataset 생성\n",
    "dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "\n",
    "# DataLoader 생성\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ilmbGqSt53D9",
   "metadata": {
    "id": "ilmbGqSt53D9"
   },
   "outputs": [],
   "source": [
    "# 검증 데이터 불러오기\n",
    "val_dir = './val'  # 검증 데이터가 저장된 디렉토리\n",
    "val_wav_files = [f for f in os.listdir(val_dir) if f.endswith('.wav')]\n",
    "\n",
    "val_features_list = []  # 검증 데이터의 특성을 저장할 리스트\n",
    "val_labels_list = []  # 검증 데이터의 라벨을 저장할 리스트\n",
    "\n",
    "for wav_file in val_wav_files:\n",
    "    label = '_'.join(wav_file.split('_')[:-1])  # 파일 이름에서 라벨 추출\n",
    "    if label not in labels_index:  # 라벨이 훈련 데이터셋의 라벨 인덱스에 없으면 스킵\n",
    "        continue  # 다음 파일로 넘어감\n",
    "\n",
    "    file_path = os.path.join(val_dir, wav_file)\n",
    "    # WAV 파일 로드\n",
    "    Fs, input_audio = wavfile.read(file_path)\n",
    "\n",
    "    # 저주파 필터링 적용\n",
    "    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n",
    "\n",
    "    # MFCC 추출\n",
    "    mfcc_features = extract_mfcc_features(filtered_audio, Fs)\n",
    "\n",
    "    label_index = labels_index[label]  # 라벨 이름을 라벨 인덱스로 변환\n",
    "    val_features_list.append(mfcc_features)  # 추출된 특성을 리스트에 추가\n",
    "    val_labels_list.append(label_index)  # 라벨 인덱스를 리스트에 추가\n",
    "\n",
    "# 가장 짧은 길이 계산\n",
    "min_length = min([len(feat) for feat in val_features_list])\n",
    "\n",
    "# 가장 짧은 길이로 모든 특성을 자르기\n",
    "trimmed_val_features_list = [feat[:min_length] for feat in val_features_list]\n",
    "\n",
    "# 특성과 라벨을 NumPy 배열로 변환\n",
    "val_features_array = np.array(trimmed_val_features_list)\n",
    "val_labels_array = np.array(val_labels_list)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "val_features_tensor = torch.tensor(val_features_array, dtype=torch.float)\n",
    "val_labels_tensor = torch.tensor(val_labels_array, dtype=torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(val_features_tensor, val_labels_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FnE1V-MSUblb",
   "metadata": {
    "id": "FnE1V-MSUblb"
   },
   "source": [
    "# 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_RG7av2hqfYb",
   "metadata": {
    "id": "_RG7av2hqfYb"
   },
   "outputs": [],
   "source": [
    "class LSTMSoundClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2):  # num_layers를 3으로 증가\n",
    "        super(LSTMSoundClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM 레이어 정의\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # 추가 완전 연결 레이어 정의\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)  # hidden_size를 늘린 새로운 완전 연결 레이어\n",
    "        self.fc2 = nn.Linear(hidden_size * 2, num_classes)  # 최종 출력을 위한 레이어\n",
    "\n",
    "        # 드롭아웃 추가 (과적합 방지)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 초기 은닉 상태와 셀 상태\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # LSTM 레이어를 통과\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # 드롭아웃 적용\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "\n",
    "        # 완전 연결 레이어를 통과\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F2bK-4N_UnD4",
   "metadata": {
    "id": "F2bK-4N_UnD4"
   },
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3FcS1Fb62EZx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10744,
     "status": "ok",
     "timestamp": 1709976522188,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "3FcS1Fb62EZx",
    "outputId": "8098be54-4b5a-43e8-f295-a47a6990dae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.8380, Train Accuracy: 0.3587, Val Loss: 1.7607, Val Accuracy: 0.5256\n",
      "Epoch 2: Train Loss: 1.4044, Train Accuracy: 0.5294, Val Loss: 1.6413, Val Accuracy: 0.5897\n",
      "Epoch 3: Train Loss: 1.2679, Train Accuracy: 0.5825, Val Loss: 1.6469, Val Accuracy: 0.5769\n",
      "Epoch 4: Train Loss: 1.1612, Train Accuracy: 0.6169, Val Loss: 1.5533, Val Accuracy: 0.6282\n",
      "Epoch 5: Train Loss: 1.0877, Train Accuracy: 0.6399, Val Loss: 1.6950, Val Accuracy: 0.6026\n",
      "Epoch 6: Train Loss: 1.0676, Train Accuracy: 0.6399, Val Loss: 1.6585, Val Accuracy: 0.6154\n",
      "Epoch 7: Train Loss: 0.9622, Train Accuracy: 0.6514, Val Loss: 1.9607, Val Accuracy: 0.6154\n",
      "Epoch 8: Train Loss: 0.9337, Train Accuracy: 0.6858, Val Loss: 1.8986, Val Accuracy: 0.6667\n",
      "Epoch 9: Train Loss: 0.8795, Train Accuracy: 0.6973, Val Loss: 2.0692, Val Accuracy: 0.6795\n",
      "Epoch 10: Train Loss: 0.8123, Train Accuracy: 0.7288, Val Loss: 1.9306, Val Accuracy: 0.7051\n",
      "Epoch 11: Train Loss: 0.7968, Train Accuracy: 0.7303, Val Loss: 1.6529, Val Accuracy: 0.7051\n",
      "Epoch 12: Train Loss: 0.7168, Train Accuracy: 0.7418, Val Loss: 1.7218, Val Accuracy: 0.6795\n",
      "Epoch 13: Train Loss: 0.7110, Train Accuracy: 0.7547, Val Loss: 1.9518, Val Accuracy: 0.6667\n",
      "Epoch 14: Train Loss: 0.6767, Train Accuracy: 0.7633, Val Loss: 1.9964, Val Accuracy: 0.6154\n",
      "Epoch 15: Train Loss: 0.6101, Train Accuracy: 0.7805, Val Loss: 2.0164, Val Accuracy: 0.6923\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "model = LSTMSoundClassifier(input_size=features_tensor.size(2), hidden_size=128, num_classes=len(labels_index)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습\n",
    "num_epochs = 50\n",
    "\n",
    "import copy\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "patience = 5  # 성능 개선이 없는 에포크 허용 수\n",
    "patience_counter = 0  # 성능 개선이 없는 에포크 카운터\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for features, labels in dataloader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_dataloader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    epoch_acc = val_correct / val_total\n",
    "\n",
    "    # 가장 좋은 모델 저장\n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0  # 성능 개선 시 카운터 리셋\n",
    "    else:\n",
    "        patience_counter += 1  # 성능 개선이 없으면 카운터 증가\n",
    "\n",
    "    print(f'Epoch {epoch+1}: Train Loss: {train_loss / len(dataloader):.4f}, Train Accuracy: {train_correct / train_total:.4f}, '\n",
    "          f'Val Loss: {val_loss / len(val_dataloader):.4f}, Val Accuracy: {val_correct / val_total:.4f}')\n",
    "\n",
    "    # 조기 종료 조건 확인\n",
    "    if patience_counter == patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# 최적의 모델로 복원(최고 성능 모델 저장 코드)\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), 'best_model_1.pth') # 모델 이름 바꿔도 됨.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "An0GmhiG2c7v",
   "metadata": {
    "id": "An0GmhiG2c7v"
   },
   "source": [
    "## test 샘플로 새 이름과 그 확률 출력하는 알고리즘!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "CXkQZt9S2gQ6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1709978201053,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "CXkQZt9S2gQ6",
    "outputId": "717cea9a-9770-4167-c862-2df95da78e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: European_Turtle_Dove_Streptopelia_turtur, with probability: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 테스트 데이터 샘플 로드\n",
    "# 저주파 필터링 및 MFCC 추출을 위한 함수 정의\n",
    "def preprocess_audio(file_path, b, a):\n",
    "    # WAV 파일 로드\n",
    "    Fs, input_audio = wavfile.read(file_path)\n",
    "\n",
    "    # 저주파 필터링 적용\n",
    "    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n",
    "\n",
    "    # MFCC 추출\n",
    "    mfcc_features = mfcc(filtered_audio, samplerate=Fs, numcep=13)\n",
    "\n",
    "    return mfcc_features\n",
    "\n",
    "# 테스트 샘플 파일 경로\n",
    "test_file_path = './test/Turtle_Dove0.wav'\n",
    "\n",
    "# 테스트 샘플에 대한 전처리 적용\n",
    "test_feature = preprocess_audio(test_file_path, b, a)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "test_feature_tensor = torch.tensor([test_feature], dtype=torch.float).to(device)\n",
    "\n",
    "# 모델 정의\n",
    "model = LSTMSoundClassifier(input_size=test_feature_tensor.size(2), hidden_size=128, num_classes=len(labels_index)).to(device)\n",
    "model.load_state_dict(torch.load('./best_model_1.pth'))\n",
    "######model.load_state_dict(torch.load('./best_model_1.pth'))\n",
    "model.eval()  # 추론 모드\n",
    "threshold = 0.9  # 확률에 대한 임계값 설정\n",
    "\n",
    "# 테스트 샘플에 대한 추론 수행\n",
    "with torch.no_grad():\n",
    "    output = model(test_feature_tensor)  # test_feature_tensor는 전처리된 테스트 데이터\n",
    "    probabilities = F.softmax(output, dim=1)  # 모델의 출력을 소프트맥스 함수를 통해 확률로 변환\n",
    "    max_probs, predicted_indices = torch.max(probabilities, dim=1)  # 각 샘플에 대한 최대 확률과 해당 인덱스를 추출\n",
    "\n",
    "    # 첫 번째 샘플에 대한 처리\n",
    "    predicted_prob = max_probs.item()  # 첫 번째 샘플의 최대 확률\n",
    "    predicted_index = predicted_indices.item()  # 첫 번째 샘플의 예측된 클래스 인덱스\n",
    "\n",
    "    # 임계값보다 낮은 확률을 가진 예측을 '불일치'로 처리\n",
    "    if predicted_prob < threshold:\n",
    "        print(\"불일치\")\n",
    "    else:\n",
    "        # 예측된 클래스의 이름을 조회\n",
    "        predicted_label = [label for label, index in labels_index.items() if index == predicted_index][0]\n",
    "        print(f\"Predicted label: {predicted_label}, with probability: {predicted_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "xXDomZC27SNJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1709978212023,
     "user": {
      "displayName": "f e",
      "userId": "13062484670338734718"
     },
     "user_tz": 0
    },
    "id": "xXDomZC27SNJ",
    "outputId": "8dbcf3f3-276f-4e26-d750-823287c0c222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999676942825317\n"
     ]
    }
   ],
   "source": [
    "print(predicted_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1BskWCocBUKy",
   "metadata": {
    "id": "1BskWCocBUKy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
