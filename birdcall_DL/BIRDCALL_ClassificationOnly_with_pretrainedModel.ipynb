{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install python_speech_features\n","!pip install pydub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1O6vghMicgl9","outputId":"83df852f-5828-4d40-ae3a-32e13ce37082","executionInfo":{"status":"ok","timestamp":1711491601758,"user_tz":0,"elapsed":31413,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python_speech_features\n","  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: python_speech_features\n","  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5870 sha256=8c99a5107a5586d2199aff82a82fe4ab9a24c3592dcd9eed45f4356935cd6d44\n","  Stored in directory: /root/.cache/pip/wheels/5a/9e/68/30bad9462b3926c29e315df16b562216d12bdc215f4d240294\n","Successfully built python_speech_features\n","Installing collected packages: python_speech_features\n","Successfully installed python_speech_features-0.6\n","Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n"]}]},{"cell_type":"code","source":["import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import os\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set the random number generator seed for all modules.\"\"\"\n","    random.seed(seed_value)  # Built-in Python random module\n","    np.random.seed(seed_value)  # NumPy\n","    torch.manual_seed(seed_value)  # PyTorch function for CPUs\n","    torch.cuda.manual_seed(seed_value)  # PyTorch function for GPUs\n","    torch.cuda.manual_seed_all(seed_value)  # PyTorch function for multi-GPUs\n","    torch.backends.cudnn.deterministic = True  # Makes cudnn algorithm deterministic\n","    torch.backends.cudnn.benchmark = False\n","    os.environ['PYTHONHASHSEED'] = str(seed_value)\n","\n","set_seed(42)  # Call with the desired seed value"],"metadata":{"id":"15DWDM5vcmgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","#import matplotlib.pyplot as plt\n","from scipy.io.wavfile import read\n","from IPython.lib.display import Audio\n","from scipy.fftpack import fft, ifft\n","from scipy.io import loadmat\n","import scipy.signal as sgnl\n","import scipy.io.wavfile as wav\n","import sys\n","import wave\n","import operator\n","import scipy\n","from python_speech_features import mfcc"],"metadata":{"id":"fkCnxXIzcqnP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","# Check if CUDA is available and set the device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Using device: {device}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oON39TY5cuXO","outputId":"9ab63479-8d56-4311-ba1d-20c929e3fa62","executionInfo":{"status":"ok","timestamp":1711491608435,"user_tz":0,"elapsed":221,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# Set the current directory\n","%cd /content/drive/My Drive/birdcall"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kH4UQtfcskx","outputId":"e18c0651-812a-4057-910a-06f84815a018","executionInfo":{"status":"ok","timestamp":1711491666196,"user_tz":0,"elapsed":57764,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/birdcall\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aPLj2sJpZw73","outputId":"9ef6990b-b01b-4fb5-da56-dde9b00ffdb7","executionInfo":{"status":"ok","timestamp":1711492648442,"user_tz":0,"elapsed":578,"user":{"displayName":"f e","userId":"13062484670338734718"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted label: Long_tailed_Duck_Clangula_hyemalis, Probability: 0.8901532888412476\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from scipy.io import wavfile\n","from python_speech_features import mfcc\n","import os\n","from scipy.io import wavfile, loadmat\n","from scipy.fft import fft, fftfreq, ifftshift\n","from scipy.fftpack import fftshift\n","from python_speech_features import mfcc\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Label to index mapping dictionary as provided\n","labels_index = {\n","    'Velvet_Scoter_Melanitta_fusca': 0,\n","    'Long_tailed_Duck_Clangula_hyemalis': 1,\n","    'Leach\\'s_Storm_Petrel_Hydrobates_leucorhous': 2,\n","    'Horned_Grebe_Podiceps_auritus': 3,\n","    'Great_Bustard_Otis_tarda': 4,\n","    'European_Turtle_Dove_Streptopelia_turtur': 5,\n","    'Black_legged_Kittiwake_Rissa_tridactyla': 6,\n","    'Balearic_Shearwater_Puffinus_mauretanicus': 7,\n","    'Atlantic_Puffin_Fratercula_arctica': 8,\n","    'Aquatic_Warbler_Acrocephalus_paludicola': 9\n","}\n","\n","# Invert the dictionary to map indices to labels\n","index_to_labels = {v: k for k, v in labels_index.items()}\n","\n","# Define the model class\n","class LSTMSoundClassifier(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_classes, num_layers=3):\n","        super(LSTMSoundClassifier, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc1 = nn.Linear(hidden_size, hidden_size * 2)\n","        self.fc2 = nn.Linear(hidden_size * 2, num_classes)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        out, _ = self.lstm(x, (h0, c0))\n","        out = self.dropout(out[:, -1, :])\n","        out = torch.relu(self.fc1(out))\n","        out = self.fc2(out)\n","        return out\n","\n","def apply_fourier_transform(input_audio, Fs):\n","    N = len(input_audio)\n","    f_transform = fftshift(fft(input_audio, N))\n","    frequencies = np.linspace(-Fs/2, Fs/2, N)\n","    return f_transform, frequencies\n","\n","import scipy.signal as signal\n","\n","def apply_lowpass_filter(input_audio, b, a):\n","    filtered_audio = signal.lfilter(b, a, input_audio)\n","    return filtered_audio\n","\n","def extract_mfcc_features(filtered_audio, Fs):\n","    mfcc_features = mfcc(filtered_audio, Fs)\n","    return mfcc_features\n","\n","# Define preprocessing function\n","def preprocess_audio(file_path, b, a):\n","    # Load WAV file\n","    Fs, input_audio = wavfile.read(file_path)\n","\n","    # Apply low-pass filtering\n","    filtered_audio = apply_lowpass_filter(input_audio, b, a)\n","\n","    # Extract MFCC\n","    mfcc_features = mfcc(filtered_audio, samplerate=Fs, numcep=13)\n","\n","    return mfcc_features\n","\n","\n","# Define the hyperparameters for the model to use\n","num_classes = 10  # Assuming there are 10 classes. Please change it to the actual number of classes.\n","input_size = 13  # Size of MFCC features (must match numcep value)\n","hidden_size = 512\n","num_layers = 3\n","\n","# Create model instance and load trained weights\n","model = LSTMSoundClassifier(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes, num_layers=num_layers)\n","model.load_state_dict(torch.load('./3.23.pth'))  # Modify 'model_path.pth' to your model's path.\n","model.eval()\n","\n","# Load filter coefficients - Set the path to 'highpass(500).mat' file accordingly\n","filter_data = loadmat('./low_filter/highpass(500).mat')  # Modify 'filter_path.mat' to your filter's path.\n","Coeffs = filter_data['ba'].astype(np.float64)\n","b = Coeffs[0, :]\n","a = 1\n","\n","# Set the path for the audio file to infer\n","test_file_path = './test/Velvet_Scoter_TEST.wav'  # Set the test audio file path.\n","\n","# Preprocess the audio file\n","test_feature = preprocess_audio(test_file_path, b, a)\n","test_feature_tensor = torch.tensor([test_feature], dtype=torch.float)\n","\n","\n","# Execute inference\n","with torch.no_grad():\n","    output = model(test_feature_tensor)\n","    probabilities = torch.softmax(output, dim=1)\n","    predicted_index = probabilities.argmax(dim=1).item()\n","    predicted_prob = probabilities.max(dim=1).values.item()\n","\n","    # Use the index to get the associated label from our dictionary\n","    predicted_label = index_to_labels[predicted_index]\n","\n","    print(f\"Predicted label: {predicted_label}, Probability: {predicted_prob}\")"]},{"cell_type":"code","source":[],"metadata":{"id":"LbJ_QFVLWiYl"},"execution_count":null,"outputs":[]}]}